# -*- coding: utf-8 -*-
"""Ict_Projectipynb (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q75W7pAIiFm8VRGb-SVsr8KtljgdrFG
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score,confusion_matrix
from sklearn.feature_selection import RFE
from imblearn.over_sampling import SMOTE
from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set plot style for consistency (use 'seaborn-v0_8' to avoid style error)
plt.style.use('seaborn-v0_8')

df =pd.read_excel("/data/E Commerce Dataset.xlsx",sheet_name='E Comm')

df.head()

df.info()

df.shape

df.describe()

df.isnull().sum()

df.nunique()

df.columns.to_list()

# Drop Irrelevant Columns
if 'CustomerID' in df.columns:
    df = df.drop('CustomerID', axis=1)

# Handle inconsistencies in categorical variables
df['PreferredLoginDevice'] = df['PreferredLoginDevice'].replace({'Phone': 'Mobile Phone'})
df['PreferredPaymentMode'] = df['PreferredPaymentMode'].replace({'CC': 'Credit Card', 'COD': 'Cash on Delivery'})

# Remove duplicate rows
df = df.drop_duplicates()

# filter out non numeric columns
df.select_dtypes(exclude=np.number).columns

# 1. Numeric columns ‚Üí fill with mean
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in num_cols:
    df[col] = df[col].fillna(df[col].mean())

# 2. Categorical columns ‚Üí fill with mode
cat_cols = df.select_dtypes(include=['object']).columns
for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(df.isnull().sum())

print("Number of duplicate rows:", df.duplicated().sum())

# Encode Categorical Columns
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])
joblib.dump(le, "label_encoder.pkl")

#outlier removal
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower) & (df[col] <= upper)]
    return df

outlier_cols = ['Tenure', 'HourSpendOnApp', 'NumberOfDeviceRegistered',
                'SatisfactionScore', 'DaySinceLastOrder']
df = remove_outliers_iqr(df, outlier_cols)

# Define Features and Target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Feature Selection using RFE (Random Forest)
model_for_rfe = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=model_for_rfe, n_features_to_select=10)
rfe.fit(X, y)
selected_features = X.columns[rfe.support_]
print("Selected Features by RFE:", list(selected_features))

X_selected = X[selected_features]
joblib.dump(list(selected_features), "selected_features.pkl")

# check class distribution counts
print(df['Churn'].value_counts())

# calculate percentage of each class
print(df['Churn'].value_counts(normalize=True) * 100)

#visualise class imbalance with a bar plot

sns.countplot(x='Churn', data=df)
plt.title('Churn Class Distribution')
plt.show()

# Balance Dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_selected, y)

print("After SMOTE, training class distribution:")
print(pd.Series(y_resampled).value_counts())

# Visualize class distribution after SMOTE
plt.figure(figsize=(6,4))
sns.countplot(x=pd.Series(y_resampled))
plt.title("Class Distribution After SMOTE")
plt.show()

#Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
joblib.dump(scaler, "scaler.pkl")

#Train and Compare Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier()
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_train_prob = model.predict_proba(X_train_scaled)[:, 1]
    y_test_prob = model.predict_proba(X_test_scaled)[:, 1]

    train_auc = roc_auc_score(y_train, y_train_prob)
    test_auc = roc_auc_score(y_test, y_test_prob)

    print(f"{name} ‚û§ Train AUC: {train_auc:.4f} | Test AUC: {test_auc:.4f}")

    results[name] = {
        'model': model,
        'train_auc': train_auc,
        'test_auc': test_auc
    }



# Detect Overfitting & Apply Tuning if Needed
for name, result in results.items():
    if result['train_auc'] - result['test_auc'] > 0.05:
        print(f"\n‚ö†Ô∏è {name} is overfitting. Applying RandomizedSearchCV tuning...")

        if name == "Random Forest":
            param_dist = {
                'n_estimators': randint(50, 200),
                'max_depth': [4, 6, 8, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2']
            }
            search = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_dist,
                                        n_iter=20, cv=5, scoring='roc_auc', n_jobs=-1, random_state=42)
        elif name == "XGBoost":
            param_dist = {
                'n_estimators': randint(50, 150),
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 4, 5],
                'subsample': [0.7, 0.8, 1.0],
                'colsample_bytree': [0.7, 0.8, 1.0]
            }
            search = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
                                        param_dist, n_iter=20, cv=5, scoring='roc_auc', n_jobs=-1, random_state=42)
        else:
            continue  # Skip tuning for others

        search.fit(X_train_scaled, y_train)
        results[name]['model'] = search.best_estimator_
        print(f"‚úÖ Tuned Best Params for {name}:", search.best_params_)

# Final Evaluation
best_model_name = max(results, key=lambda x: results[x]['test_auc'])
final_model = results[best_model_name]['model']

y_pred = final_model.predict(X_test_scaled)
y_prob = final_model.predict_proba(X_test_scaled)[:, 1]

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Test ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Plot
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix: {best_model_name}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Save Final Model
joblib.dump(final_model, "final_churn_model.pkl")

